To identify an optimal duration, we trained SmolVLM on average video lengths ranging from 1.5 to 3.5 minutes. Figure 7 (right) demonstrates clear performance improvements for both video and image benchmarks as video durations approached approximately 3.5 minutes, likely due to more effective cross-modal feature learning. Extending video duration beyond 3.5 minutes yielded minimal further gains, indicating diminishing returns relative to the added computational expense. Thus, moderately extending video sequences enhances performance significantly in smaller models, whereas overly long sequences do not proportionally justify their computational cost. Finding 9. Moderately increasing video duration during training improves both video and image task performance in compact VLMs. 4 Experimental Results We construct three variants of SmolVLM, tailored to different computational environments: •SmolVLM- 256M: Our smallest model, combining the 93M SigLIP-B/ 16and the SmolLM 2-135M (Allal et al., 2025). Operating on <1GB GRAM makes it ideal for resource-constrained edge applications. •SmolVLM- 500M: A mid-range model with the same 93M SigLIP-B/ 16paired with the larger SmolLM 2-360M. Balancing memory efficiency and performance, it is suitable for moderate-resource edge devices. •SmolVLM- 2.2B: The largest variant, with a 400M SigLIP-SO 400M and a 1.7B-parameter SmolLM 2backbone. This model maximizes performance while remaining deployable on higher-end edge systems. 7 Vision Training DataImage86% Text 14% OCR& Documents 48% Captioning 14% ChartUnderstanding 12% Reasoning & Logic 9% Table Understanding 9% Visual Q&A 8% Reasoning & Logic 79% Geneneral Knowledge 21% Video Fine-Tuning DataImage 35% Video 33% Text 20%Multi-Image 12% Captioning 37% Reasoning & Logic 26% Document Understanding 23% Visual Q&A 14% Visual Description& Captioning76% Temporal Underst. 18% Narrative6% Information Seeking 60% GeneralKnowledge 25% Reasoning & Logic 15% Visual Q&A 100%Figure 8 ∣Data Details. Training dataset details for Vision (Left)and video (Right), broken down by modality and sub-categories. 4.1 Training Data Model training proceeds in two stages: (1) a vision