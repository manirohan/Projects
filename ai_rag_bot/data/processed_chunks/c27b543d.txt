(Gemma 3) reserve vision capabilities for large-scale models. Even PaliGemma (Beyer et al., 2024), initially efficiency-focused, scaled up significantly in its second release (Steiner et al., 2024). In contrast, Moondream (Korrapati, 2024) keeps focusing on improving performance while maintaining efficiency, and H 2OVL-Mississippi (Galib et al., 2024) explicitly targets on-device deployment. Efficient processing is particularly critical for video understanding tasks, exemplified by Apollo (Zohar et al., 2024b), where memory management is essential. Furthermore, reasoning LLMs generate more tokens during inference, compounding computational costs (DeepSeek-AI, 2025; OpenAI et al., 2024). Therefore, efficiency per token becomes vital to ensure models remain practical for real-world use. Our contributions are: •Compact yet Powerful Models : We introduce SmolVLM, a family of powerful small-scale multimodal models, demonstrating that careful architectural design can substantially reduce resource requirements without sacrificing capability. •EfficientGPUMemoryUsage : Oursmallestmodelrunsinferenceusinglessthan1GBGPURAM,significantly lowering the barrier to on-device deployment. •Systematic Architectural Exploration : We comprehensively investigate the impact of architectural choices, including encoder-LM parameter balance, tokenization methods, positional encoding, and training data composition, identifying critical factors that maximize performance in compact VLMs. •Robust Video Understanding on Edge Devices : We demonstrate that SmolVLM models generalize effectively to video tasks, achieving competitive scores on challenging benchmarks like Video-MME, highlighting their suitability for diverse multimodal scenarios and real-time, on-device applications. •Fully Open-source Resources : To promote reproducibility and facilitate further research, we release all model weights, datasets, code, and a mobile application showcasing inference on a smartphone. 2 0 500 1000 1500 2000 Total Parameters (M)0.300.350.400.450.500.55Mean CIDEr and VQA accuracy Encoder Params 93M 428M 2 8 16 Context Length (k)0.300.350.400.450.500.55 256M 500M Model Size0.300.350.400.450.500.55 Pixel Shuffle 2 4 1 2 4 8 Frame Averaging Factor0.450.460.470.480.490.500.51 Figure 3 ∣Performance analysis of SmolVLM configurations. (Left)Impact of vision encoder and language model sizes. Smaller language models ( 135M) benefit less from