vs. OpenCompass-Video: learned tokens dominate the higher-scoring region, especially in image-intensive tasks. 3 Smol Instruction Tuning Smol instruction tuning requires careful vision (§3.1) and text tokenization (§3.2), alongside unified methods for multimodal modeling under tight compute constraints. Learned positional tokens and structured prompts stabilize training and improve OCR, but data composition remains crucial: reusing LLM instruction datasets negatively impacts small VLMs (§3.3), excessive Chain-of-Thought data overwhelms limited capacity (§3.4), and moderate video sequence lengths balance efficiency and performance (§3.5). Collectively, these insights highlight targeted strategies essential for effectively scaling multimodal instruction tuning to SmolVLMs. 3.1 Learned Tokens vs. String A primary design consideration in SmolVLM involves encoding split sub-image positions effectively. Initially, we attempted to use simple string tokens (e.g., <row_1_col_2> ), which caused early training plateaus—termed the “OCR loss plague”—characterized by sudden loss drops without corresponding improvements in OCR performance (Figure 5, left and middle). To address instability during training, we introduced positional tokens, significantly improving training convergence and reducing stalls. Although larger models were relatively robust to using raw string positions, smaller models benefited substantially from positional tokens, achieving notably higher OCR accuracy and improved generalization across tasks. Figure 5 (center) shows that learned positional tokens consistently outperform naive string positions on multiple image and text benchmarks. Additionally, Figure 5 (right) illustrates that models leveraging learned tokens consistently score higher in both OpenCompass-Image and OpenCompass-Video evaluations, underscoring the effectiveness of structured positional tokenization in compact multimodal models. Finding 5. Learned positional tokens outperform raw text tokens for compact VLMs. 3.2 Structured Text Prompts and Media Segmentation We evaluated how system prompts and explicit media intro/outro prefixes incrementally improve SmolVLM’s performance on image (left) and video (right) benchmarks, as shown in Figure 6. Each violin plot represents three checkpoints for a given configuration. System Prompts. We prepend concise instructions