compact VLMs. Specifically, we pair three SmolLM2 variants ( 135M,360M, and 1.7B parameters) with two SigLIP encoders: a compact 93M SigLIP-B/ 16and a larger 428M SigLIP-SO 400M. Typically, larger VLMs disproportionately allocate parameters to the LM; however, as the LM is scaled down, this is no longer the case. Figure 3 (left) confirms that performance declines significantly when using a large encoder with the smallest LM ( 135M), highlighting an inefficient encoder-LM balance. At an intermediate LM scale ( 360M), the larger encoder improves performance by 11.6%, yet this comes with a substantial 66% increase in parameters, making the compact encoder preferable. Only at the largest LM scale ( 1.7B), the larger encoder represents just a 10% parameter increase. Finding 1. Compact multimodal models benefit from a balanced encoder-LM parameter allocation, making smaller vision encoders preferable for efficiency. 2.2 How can we efficiently pass the images to the Language Model? Following Laurençon et al. (2024), we adopt a self-attention architecture in which visual tokens from the vision encoder are concatenated with textual tokens and jointly processed by a language model (e.g., FROMAGe (Koh et al., 2023), BLIP- 2(Li et al., 2023a)). This design requires significantly more context than the 2k-token limit used in SmolLM 2, as a single 512×512image encoded with SigLIP-B/ 16requires 1024tokens. To address this, we extended the context capacity by increasing the RoPE base from 10k to 273k, following Liu et al. (2024c), and fine-tuned the model on a mix of long-context data (Dolma books (Soldaini et al., 2024), The Stack (Kocetkov et al., 2022)) and short-context sources (FineWeb-Edu (Penedo et al., 2024), DCLM (Li et al., 2024a), and math from SmolLM2). 3 Figure 4 ∣Pixel shuffle. Rearranges encoded images, trading spatial resolution for increased channel depth. This reduces visual token count while preserving information density. While