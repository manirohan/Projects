increasing memory requirements. On the smaller end, models like PaliGemma, Moondream2, and MiniCPM-V demonstrate impressive mul- timodal capabilities within constrained parameter budgets. PaliGemma (Team et al., 2024), with just 3B parameters (400M vision encoder from SigLIP-So (Zhai et al., 2023) and 2B Gemma language model), effectively covers a wide range of multimodal tasks. However, its condensed visual interface can limit detailed visual analysis. Moondream2, at merely 1.8B parameters, pairs SigLIP visual features with Microsoft’s Phi-1.5 language model (Li et al., 2023b), showcasing competitive performance on tasks such as image description, OCR, counting, and classification, ideal for edge and mobile applications. MiniCPM-V (Hu et al., 2024), specifically designed for on-device scenarios, integrates a 400M vision encoder and a 7.5B language model via a perceiver-style adapter. This compact model notably achieves GPT-4V-level performance on selected benchmarks. Deepseek VL and Deepseek VL2 (Lu et al., 2024a; Wu et al., 2024), spanning 2–7B and 4–27B parameters respectively, further illustrate the growing focus on efficient yet powerful multimodal models suitable for resource-constrained environments. Collectively, these models demonstrate the increasing feasibility of deploying effective, real-time multimodal AI in practical scenarios. 12 5.3 Multimodal Tokenization and Compression Strategies Efficient tokenization significantly reduces computational and memory demands in Vision-Language Models (VLMs). Early methods, encoding every pixel or patch individually, resulted in lengthy sequences—196 tokens for a 224 ×224 image at 16 ×16 resolution. Recent strategies compress visual data while preserving essential details. Learned modules like Perceiver Resamplers (Jaegle et al., 2021) used by Flamingo and Idefics2 (Alayrac et al., 2022b; Laurençon et al., 2024), and BLIP-2’s Q-Former (Li et al., 2023a), compress inputs into a small set of latent tokens. While effective in shortening sequences, these methods may limit performance on fine-grained tasks like OCR (Singh et al., 2019; Biten et al., 2019). Spatial compression via patch pooling