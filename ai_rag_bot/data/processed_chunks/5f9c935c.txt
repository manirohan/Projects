al., 2022b; Laurençon et al., 2024), and BLIP-2’s Q-Former (Li et al., 2023a), compress inputs into a small set of latent tokens. While effective in shortening sequences, these methods may limit performance on fine-grained tasks like OCR (Singh et al., 2019; Biten et al., 2019). Spatial compression via patch pooling and pixel shuffle is increasingly popular. InternVL v1.5 and Idefics3 (Chen et al., 2024c,b; Laurençon et al., 2023) use 2 ×2 pixel-shuffle, reducing token counts fourfold while maintaining OCR capability. Models like Qwen-VL-2 (Wang et al., 2024a) adopt multi-scale representations and selective token dropping via convolutional and Transformer modules. Adaptive methods, such as image tiling in UReader and DocOwl, dynamically adjust token counts based on task complexity, sacrificing some global context. 5.4 Video-Capable Vision-Language Models Extending vision-language models (VLMs) from images to videos significantly increases complexity due to temporal dimensions, expanding token counts and computational demands. Early models, such as Video- LLaVA (Lin et al., 2023a), unified image and video training, aligning video frame features with static images and substantially outperforming predecessors like Video-ChatGPT (Maaz et al., 2023) on benchmarks including MSRVTT (Xu et al., 2016), MSVD (Chen and Dolan, 2011), TGIF (Li et al., 2016), and ActivityNet (Caba Heilbron et al., 2015). Meanwhile, Video-STaR (Zohar et al., 2024a) introduced the first self-training approach that leverages existing labeled video datasets for instruction tuning of Large Multimodal Models. Recent models enhance efficiency and effectiveness in handling long-form video content. Temporal Preference Optimization (TPO) (Li et al., 2025a) employs self-training with localized and comprehensive temporal grounding, improving benchmarks like LongVideoBench, MLVU, and Video-MME. Oryx MLLM (Liu et al., 2024g) dynamically compresses visual tokens via its OryxViT encoder, balancing efficiency and precision across tasks. VideoAgent (Wang et al., 2024b) models long-form video understanding as a decision-making process, utilizing a large language model (LLM)