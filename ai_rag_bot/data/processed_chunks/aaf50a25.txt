worse than new text SFT data. 3.4 Optimizing Chain-of-Thought Integration for Compact Models Chain-of-Thought (CoT) prompting, which exposes models to explicit reasoning steps during training, generally enhances reasoning capabilities in large models. However, its effect on smaller multimodal architectures remains unclear. To investigate this, we varied the proportion of CoT data integrated into the Mammoth dataset (Yue et al., 2024b), covering text, image, and video tasks. Figure 7 (middle) shows that incorporating a minimal fraction (0.02–0.05%) of CoT examples slightly improved performance, but higher proportions 6 Video Image0.350.400.450.500.55OpenCompass Average-3.7% -6.5%Without SmolTalk With SmolTalk 0.0 0.2 0.4 0.6 CoT Data0.450.50 Video Image 1.5 min 2.5 min 3.5 min Average Video Duration0.350.400.450.500.55 Video ImageFigure7 ∣ImpactofTrainingStrategiesonSmol-ScaleMultimodalModels. (Left)Reusing text data from LLM-SFT (SmolTalk ) reduces both image and video scores in smaller models. (Middle) A minimal fraction ( 0.02%–0.05%) of Chain-of-Thought (CoT) data yields optimal results, while heavier CoT usage degrades performance. (Right)Increasing average video duration beyond 3.5min leads to diminished returns for both image and video tasks. markedly degraded results, especially in image tasks. These observations suggest that excessive reasoning- oriented textual data can overwhelm the limited capacity of smaller VLMs, thereby compromising their visual representation capabilities. Consequently, compact models benefit most from very sparse inclusion of CoT data rather than the extensive use typically beneficial in larger-scale architectures. Finding 8. Excessive CoT data harms compact model performance. 3.5 Impact of Video Sequence Length on Model Performance Increasing video duration during training offers richer temporal context but comes at a greater computational cost. To identify an optimal duration, we trained SmolVLM on average video lengths ranging from 1.5 to 3.5 minutes. Figure 7 (right) demonstrates clear performance improvements for both video and image benchmarks as video durations approached approximately 3.5 minutes, likely due to more effective cross-modal feature learning. Extending video duration beyond