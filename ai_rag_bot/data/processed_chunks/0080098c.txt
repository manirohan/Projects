and WorldSense. SmolVLM- 2.2B notably excels at Video-MME ( 52.1) and WorldSense ( 36.2), outperforming significantly larger models such as Qwen 2VL-7B ( 32.4on WorldSense), showcasing strong capabilities in complex multimodal video comprehension tasks. The SmolVLM- 500M variant also demonstrates robust performance, achieving competitive scores on TempCompass ( 49.0) and WorldSense (30.6), highlighting sophisticated temporal reasoning and real-world visual understanding at a scale ideal for edge-device deployment. Despite their compact parameter counts, SmolVLM variants consistently balance efficient resource use with impressive accuracy, reinforcing their suitability for resource-constrained scenarios. 10 4.4 On-Device Performance To comprehensively assess the deployment practicality of SmolVLM, we benchmarked its throughput across varying batch sizes on two representative hardware platforms: NVIDIA A 100and NVIDIA L 4GPUs (see Figure 9). Our evaluations highlight SmolVLM’s suitability for on-device and edge deployment scenarios. On the A 100GPU, the smallest SmolVLM-256M variant achieves impressive throughput, scaling from 0.8 examples per second at batch size 1to16.3examples per second at batch size 64. The 500M variant similarly scales from 0.7to9.9examples per second, while the largest 2.2B variant demonstrates more modest scaling (0.6to1.7examples per second), indicative of its higher computational demands. Evaluations on the L 4GPU further emphasize SmolVLM’s edge compatibility. Here, the 256M variant reaches peak throughput at 2.7examples per second with batch size 8, subsequently diminishing due to memory constraints. The 500M and 2.2B variants peak at lower batch sizes ( 1.4and 0.25examples per second, respectively), underscoring their efficiency even under more restrictive hardware conditions. Finally, we accompany the release with several optimized ONNX (Open Neural Network Exchange) exports, facilitating cross-platform compatibility and broadening deployment opportunities across consumer-grade hardware targets. Notably, we demonstrate the ability to efficiently run these models locally within a browser environment via WebGPU, with the 256M variant achieving up to 80decode tokens per second on a 14-inch MacBook