by providing accurate answers to clinical questions based on visual data. This capability is particularly valuable in healthcare settings where quick, reliable image interpretation is critical, yet computational resources may be limited. 5 Related Work 5.1 First-Generation Vision-Language Models Early multimodal models achieved significant progress primarily by scaling parameters, but their high computational demands limited practical deployment. For instance, Flamingo (Alayrac et al., 2022b), an 80B-parameter Vision-Language Model (VLM), integrated a frozen 70B-parameter LM (Hoffmann et al., 2022) with a vision encoder employing gated cross-attention and a Perceiver Resampler (Jaegle et al., 2021) for 11 efficient token compression. Despite state-of-the-art few-shot capabilities without task-specific fine-tuning, Flamingo’s large scale posed significant deployment challenges. Hugging Face’s Idefics (Laurençon et al., 2023) adopted Flamingo’s architecture, offering models at both 9B and 80B parameters, further exemplifying the approach of large-scale multimodal training. In contrast, BLIP-2 (Li et al., 2023a) proposed a more parameter-efficient, modular design by freezing both the vision encoder and language model, introducing instead a lightweight Query Transformer (Q-Former) that translates visual features into language-compatible tokens. This approach significantly reduced trainable parameters, surpassing Flamingo’s performance on VQA tasks (Antol et al., 2015; Goyal et al., 2017) with roughly 54 times fewer trainable parameters, thus paving the way toward more efficient multimodal architectures. Similarly, LLaVA (Large Language-and-Vision Assistant) (Liu et al., 2023) connected a pretrained CLIP (Rad- ford et al., 2021) ViT image encoder to a LLaMA/Vicuna language backbone (Touvron et al., 2023; Zheng et al., 2024), fine-tuning the combined model on instruction-following datasets. Resulting in a 13B-parameter multimodal chatbot with GPT-4V-like capabilities (Achiam et al., 2023), LLaVA achieved notable visual conversational performance. However, despite being smaller and faster than Flamingo, it still demands substantial GPU memory for real-time interaction and inherits the limitations of the underlying language model’s context window (typically 2048