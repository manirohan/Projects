Q&A 14% Visual Description& Captioning76% Temporal Underst. 18% Narrative6% Information Seeking 60% GeneralKnowledge 25% Reasoning & Logic 15% Visual Q&A 100%Figure 8 ∣Data Details. Training dataset details for Vision (Left)and video (Right), broken down by modality and sub-categories. 4.1 Training Data Model training proceeds in two stages: (1) a vision stage, and (2) a video stage. The vision training stage uses a new mixture of the datasets used in Laurençon et al. (2024), to which we added MathWriting (Gervais et al., 2024). The mixture was balanced to emphasize visual and structured data interpretation while maintaining the focus on reasoning and problem-solving capabilities. The visual components comprise document understanding, captioning, and visual question answering (including 2% dedicated to multi-image reasoning), chart understanding, table understanding, and visual reasoning tasks. To preserve the model’s performance in text-based tasks, we retained a modest amount of general knowledge Q&A and text-based reasoning & logic problems, which incorporate mathematics and coding challenges. The video fine-tuning stage maintains 14% of text data and 33% of video to achieve optimal performance, following the learnings of Zohar et al. (2024b). For video, we sample visual description and captioning from LLaVA-video- 178k (Zhang et al., 2024), Video-STAR (Zohar et al., 2024a), Vript (Yang et al., 2024), and ShareGPT4Video (Chen et al., 2023), temporal understanding from Vista- 400k (Ren et al., 2024), and narrative comprehension from MovieChat (Song et al., 2024) and FineVideo (Farré et al., 2024). Multi-image data was sampled from M 4-Instruct (Liu et al., 2024a) and Mammoth (Guo et al., 2024). The text samples were sourced from (Xu et al., 2024). For a more granular description, Figure 8 provides a detailed overview of the training data distribution used in both our vision and video fine-tuning stages. 4.2 Evaluation details We evaluated SmolVLM using VLMEvalKit (Duan et al., 2024)