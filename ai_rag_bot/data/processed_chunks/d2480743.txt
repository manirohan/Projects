requires only 0.8GB of VRAM for the 256M variant, 1.2GB for the 500M, and 4.9GB for the 2.2B—dramatically lower than the 27.7GB required by MolmoE-A 1B-7B. Even compared to models of similar parameter scales, SmolVLM is notably more efficient: Qwen 2VL-2B requires 13.7GB VRAM and InternVL2- 2B requires 10.5GB VRAM, highlighting that parameter count alone does not dictate compute requirements. At batch size 64, memory usage for SmolVLM remains practical: 15.0GB (256M),16.0GB ( 500M), and 49.9GB ( 2.2B). These results highlight SmolVLM’s substantial advantages for deployment in GPU-constrained environments. Overall Gains from Scaling. Increasing SmolVLM’s parameter count consistently yields substantial per- formance improvements across all evaluated benchmarks. The largest model ( 2.2B) achieves the highest overall score at 59.8%, followed by the intermediate 500M variant ( 51.0%) and the smallest 256M variant (44.0%). Notably, even the smallest SmolVLM- 256M significantly surpasses the much larger Idefics 80B model (see Fig. 1) on nearly all benchmarks, emphasizing effective vision capabilities at modest scales. The few exceptions—particularly MMMU ( 29.0%vs.42.3%) and AI2D ( 46.4%vs.56.3%)—highlight benchmarks where strong linguistic reasoning from a large language backbone remains crucial. Intriguingly, visually oriented tasks such as OCRBench also benefit markedly from scaling language model capacity, with a nearly 10-point improvement when moving from 256M (52.6%) to 500M (61.0%). These results underscore that 9 Figure 9 ∣SmolVLM on edge device. (Left)Examples of the HuggingSnap app, where SmolVLM can run locally, on the device, on consumer phones. For example, interactions can be done using a mobile interface to detect objects and answer questions. (Right)Throughput in tokens per second on NVIDIA A 100GPUs(top)and different consumer personal computers (bottom) across different batch sizes and model variants. larger language models provide enhanced context management and improved multimodal reasoning, benefiting both language-intensive and vision-centric tasks. Comparison with Other Compact VLMs. Figure 1 situates