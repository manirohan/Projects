at significantly smaller scales. Code gitub/huggingface Blog blog/smolvlm2 Weights community/smol-research VLM Browser spaces/smolvlm-webgpu Demo spaces/smolvlm2 HuggingSnap apple/huggingsnap 1arXiv:2504.05299v1 [cs.AI] 7 Apr 2025 Image SplittingVision Encoder Pixel ShuffleLinear Layer SmolLM2(1, 1) (1,2) (1, 3) (2, 1) (2,2) (2, 3) What is this ?TokenizerHugging FaceThe emoji ! Image Input What is this?Figure 2 ∣SmolVLM Architecture . Images are split into subimages, frames are sampled from videos, and then encoded into visual features. These features are first rearranged via a pixel-shuffle operation, then mapped into the LLM input space as visual tokens using an MLP projection. Visual tokens are then concatenated/interleaved with text embeddings (orange/red). This combined sequence is passed to the LLM for text output. 1 Introduction Vision-Language Models (VLMs) have rapidly advanced in capability and adoption (Achiam et al., 2023; Bai et al., 2023; Beyer et al., 2024; Chen et al., 2024c; McKinzie et al., 2024), driving breakthroughs in cross-modal reasoning (Liu et al., 2024a, 2023) and document understanding (Appalaraju et al., 2021; Faysse et al., 2024a; Livathinos et al., 2025; Nassar et al., 2025a). However, these improvements typically entail large parameter counts and high computational demands. Since early large-scale VLMs like Flamingo (Alayrac et al., 2022a) and Idefics (Laurençon et al., 2023) demonstrated capabilities with 80B parameters, new models have slowly appeared at smaller sizes. However, thesemodelsoftenretainhighmemorydemandsduetoarchitecturaldecisionsmadefortheirlargercounterparts. For instance, Qwen2-VL (Wang et al., 2024a) and InternVL 2.5 (Chen et al., 2024b) offer smaller variants (1B-2B), but retain significant computational overhead. Conversely, models from Meta (Dubey et al., 2024) and Google (Gemma 3) reserve vision capabilities for large-scale models. Even PaliGemma (Beyer et al., 2024), initially efficiency-focused, scaled up significantly in its second release (Steiner et al., 2024). In contrast, Moondream (Korrapati, 2024) keeps focusing on improving performance while maintaining efficiency, and H 2OVL-Mississippi (Galib et al., 2024) explicitly targets on-device