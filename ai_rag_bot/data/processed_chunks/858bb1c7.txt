and comprehensive temporal grounding, improving benchmarks like LongVideoBench, MLVU, and Video-MME. Oryx MLLM (Liu et al., 2024g) dynamically compresses visual tokens via its OryxViT encoder, balancing efficiency and precision across tasks. VideoAgent (Wang et al., 2024b) models long-form video understanding as a decision-making process, utilizing a large language model (LLM) as an agent to identify and compile crucial information for question answering iteratively. VideoLLaMA3 (Zhang et al., 2025) adapts its vision encoder for variable resolutions and uses multi-task fine-tuning to enhance video comprehension. Video-XL (Shu et al., 2024) introduces Visual Summarization Tokens (VST) and curriculum learning for efficient handling of hour-scale videos. Similarly, Kangaroo (Liu et al., 2024b) utilizes curriculum training to scale input resolution and frame count progressively, achieving top performance on diverse benchmarks. Apollo (Zohar et al., 2024b) recently made an in-depth exploration of Video-LMMs and showed the architecture and training schedule that most affect performance. In so doing, it showed the remarkable efficiency gains that can be made during training and inference. Apollo achieved state-of-the-art results with modest parameter sizes on benchmarks such as LongVideoBench, MLVU, and Video-MME (Zhou et al., 2024; Fu et al., 2024). 6 Conclusion We introduced SmolVLM , a family of memory-efficient Vision-Language Models ranging from 256M to 2.2B parameters. Remarkably, even our smallest variant requires less than 1GB of GPU memory yet surpasses state- of-the-art 80B-parameter models from just 18 months ago (Laurençon et al., 2023). Our findings emphasize a critical insight: scaling down large VLM architectures optimized under resource-rich conditions results in disproportionately high memory demands during inference with little advantage over specialized architectures. By contrast, SmolVLM’s design philosophy explicitly prioritizes compact architectural innovations, aggressive but careful tokenization methods, and efficient training strategies, enabling powerful multimodal capabilities at a fraction of the computational cost. All model weights, training datasets, and training