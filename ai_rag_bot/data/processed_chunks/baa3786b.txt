Understanding50.2% 60.2% 73.0% 61.5% MolmoE-A1B-7B DocVQA (Mathew et al., 2021) Document Understanding58.3% 70.5% 80.0% 77.7% MolmoE-A1B-7B ScienceQA (Lu et al., 2022) High-school Science73.8% 80.0% 89.6% 87.5% MolmoE-A1B-7B Multi-taskMMMU (Yue et al., 2024a) College-level Multidiscipline29.0% 33.7% 42.0% 33.9% MolmoE-A1B-7B MathVista (Lu et al., 2024b) General Math Understanding35.9% 40.1% 51.5% 37.6% MolmoE-A1B-7B MMStar (Chen et al., 2024a) Multidisciplinary Reasoning34.6% 38.3% 46.0% 43.1% MolmoE-A1B-7B VideoVideo-MME (Fu et al., 2024) General Video Understanding33.7% 42.2% 52.1% 45.0% InternVL2-2B MLVU (Zhou et al., 2024) MovieQA + MSRVTT-Cap40.6% 47.3% 55.2% 48.2% InternVL2-2B MVBench (Li et al., 2024b) Multiview Reasoning32.7% 39.7% 46.3% 60.2% InternVL2-2B WorldSense (Hong et al., 2025) Temporal + Physics29.7% 30.6% 36.2% 32.4% Qwen2VL-7B TempCompass (Liu et al., 2024d) Temporal Understanding43.1% 49.0% 53.7% 53.4% InternVL2-2B Average Across Benchmarks 44.0% 51.0% 59.8% – RAM UsageBatch size = 1 0.8 GB 1.2 GB 4.9 GB 27.7 GB MolmoE-A1B-7B batch size = 64 15.0 GB 16.0 GB 49.9 GB – Table 1 ∣Benchmark comparison of SmolVLM variants across vision-language tasks. Performance of SmolVLM models at three scales (256M, 500M, and 2.2B parameters) compared to efficient open-source models on single-image, multi-task, and video benchmarks. SmolVLM models demonstrate strong accuracy while maintaining significantly lower RAM usage, highlighting their computational efficiency for resource-constrained multimodal scenarios. activated parameters (Deitke et al., 2024)(MolmoE-A 1B-7B) for vision tasks and InternVL 2-2B Chen et al. (2024c) for video tasks. A broader array of competing models are shown in Fig. 1. Efficiency and Memory Footprint. SmolVLM demonstrates remarkable computational efficiency compared to significantly larger models. Single-image inference requires only 0.8GB of VRAM for the 256M variant, 1.2GB for the 500M, and 4.9GB for the 2.2B—dramatically lower than the 27.7GB required by MolmoE-A 1B-7B. Even compared to models of similar parameter scales, SmolVLM is notably more efficient: Qwen 2VL-2B requires 13.7GB VRAM and InternVL2- 2B requires 10.5GB VRAM,