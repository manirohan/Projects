VQA accuracy Encoder Params 93M 428M 2 8 16 Context Length (k)0.300.350.400.450.500.55 256M 500M Model Size0.300.350.400.450.500.55 Pixel Shuffle 2 4 1 2 4 8 Frame Averaging Factor0.450.460.470.480.490.500.51 Figure 3 ∣Performance analysis of SmolVLM configurations. (Left)Impact of vision encoder and language model sizes. Smaller language models ( 135M) benefit less from larger vision encoders (SigLIP-SO- 400M,428M) compared to SigLIP-B/ 16(93M), while larger language models gain more from powerful encoders. (Middle-left) Performance significantly improves with increased context lengths ( 2k to 16k tokens). (Middle-right) Optimal pixel shuffle factor (PS=2 vs. PS=4) varies by model size. (Right)Frame averaging reduces video performance, with a rapid decline as more frames are averaged. Metrics average CIDEr (captioning) and accuracy (visual question answering). 2 Smoller Model Architecture We systematically explore design choices for small multimodal models based on the architecture in Figure 2, where encoded images are pooled and projected into a SmolLM2 backbone. We first analyze optimal compute allocation, showing smaller vision encoders complement compact LMs (§2.1). Extending context length enables higher image resolutions at minimal overhead (§2.2), and pixel shuffling reduces visual tokens further. Finally, we efficiently handle high-resolution images and videos via document-specific image splitting and targeted token compression (§2.3). Together, these approaches yield a unified, performant, and cost-effective recipe for tiny LMMs. 2.1 How to assign compute between vision and language towers? VLMs utilize vision encoders (see Figure 2) to generate ‘vision tokens’ that are then fed into an LM. We investigate optimal capacity allocation between vision encoders and language models (LMs) in compact VLMs. Specifically, we pair three SmolLM2 variants ( 135M,360M, and 1.7B parameters) with two SigLIP encoders: a compact 93M SigLIP-B/ 16and a larger 428M SigLIP-SO 400M. Typically, larger VLMs disproportionately allocate parameters to the LM; however, as the LM is scaled down, this is no longer the case. Figure