datasets. Resulting in a 13B-parameter multimodal chatbot with GPT-4V-like capabilities (Achiam et al., 2023), LLaVA achieved notable visual conversational performance. However, despite being smaller and faster than Flamingo, it still demands substantial GPU memory for real-time interaction and inherits the limitations of the underlying language model’s context window (typically 2048 tokens). Recent research has actively explored various design choices, training strategies, and data configurations to enhance Vision-Language Models (VLMs). For instance, Idefics2 (Laurençon et al., 2024) introduced architectural and training-data improvements compared to its predecessor, advancing open-source VLM capabilities. Concurrently, Cambrian1 (Tong et al., 2024) examined fundamental design principles and scaling behaviors, aiming for more efficient architectures. Projects like Eagle (Shi et al., 2024) and its successor Eagle2 (Li et al., 2025b) have optimized specific architectural components, targeting improved performance and efficiency. Additionally, recent efforts such as Apollo (Zohar et al., 2024b) extend multimodal architectures from static images to video understanding, further enriching the diversity of approaches. 5.2 Efficiency-Focused Vision-Language Models Largermodels, suchasInternVL(Chenetal.,2024c,b)andQwen-VL(Baietal.,2023,2025;Wangetal.,2024a), introduced architectural innovations for improved computational efficiency. InternVL aligns a 6B-parameter vision transformer (ViT) with an 8B-parameter language "middleware," forming a 14B-parameter model that achieves state-of-the-art results across multiple vision and multimodal tasks. This balanced architecture narrows the modality gap, enabling robust multimodal perception and generation capabilities. Similarly, Qwen-VL integrates a Qwen language model with specialized visual modules, leveraging captioned bounding- box data to enhance visual grounding and text recognition capabilities. Despite its strong multilingual and multimodal performance, Qwen-VL generates exceptionally long token sequences for high-resolution inputs, increasing memory requirements. On the smaller end, models like PaliGemma, Moondream2, and MiniCPM-V demonstrate impressive mul- timodal capabilities within constrained parameter budgets. PaliGemma (Team et al., 2024), with just 3B parameters (400M vision encoder from SigLIP-So (Zhai et al., 2023) and 2B Gemma language model), effectively covers a wide range