{
  "id": "2504.05278v1",
  "title": "The challenge of uncertainty quantification of large language models in\n  medicine",
  "abstract": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge.",
  "authors": [
    "Zahra Atf",
    "Seyed Amir Ahmad Safavi-Naini",
    "Peter R. Lewis",
    "Aref Mahjoubfar",
    "Nariman Naderi",
    "Thomas R. Savage",
    "Ali Soroush"
  ]
}